{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8aeac7f",
   "metadata": {},
   "source": [
    "<h1>Metacritic Scrapers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac80c5a",
   "metadata": {},
   "source": [
    "<h2>Extracting Movie Titles, Ratings, and Slugs + Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa925ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "base_url = \"https://www.metacritic.com/browse/movie/?genre=action&genre=adventure&genre=animation&genre=comedy&genre=drama&genre=fantasy&genre=horror&genre=romance&genre=sci---fi&genre=thriller&releaseYearMin=2014&releaseYearMax=2023&page=\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "movie_list = []\n",
    "movie_slugs = []\n",
    "\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract movie information based on the data-title attribute\n",
    "    movies = soup.select('.c-finderProductCard_title[data-title]')\n",
    "\n",
    "    for movie in movies:\n",
    "        title = movie['data-title']\n",
    "        score = movie.find_next(class_='c-siteReviewScore_xsmall').text.strip()\n",
    "\n",
    "        movie_list.append([title, score])\n",
    "    \n",
    "    # Find all elements with class 'c-finderProductCard'\n",
    "    card_elements = soup.find_all(class_='c-finderProductCard')\n",
    "\n",
    "    # Extract movie IDs from href attribute and append to the list\n",
    "    for card_element in card_elements:\n",
    "        link_element = card_element.find('a', href=True)\n",
    "        if link_element:\n",
    "            href_value = link_element['href']\n",
    "            movie_slug = href_value.split('/')[-2]\n",
    "            movie_slugs.append(movie_slug)\n",
    "\n",
    "# Iterate through and scrape all 196 pages\n",
    "for page_number in range(1, 197):\n",
    "    url = f\"{base_url}{page_number}\"\n",
    "    scrape_page(url)\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add slugs to movie list elements\n",
    "combined_list = [title_score + [slug] for title_score, slug in zip(movie_list, movie_slugs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates \n",
    "\n",
    "# Set to keep track of unique movies\n",
    "unique_slugs = set()\n",
    "\n",
    "# List to store unique occurrences\n",
    "unique_occurrences = []\n",
    "\n",
    "for movie_info in combined_list:\n",
    "    slug = movie_info[2]\n",
    "    if slug not in unique_slugs:\n",
    "        unique_occurrences.append(movie_info)\n",
    "        unique_slugs.add(slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22277ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "\n",
    "import csv\n",
    "csv_filename = \"metacritic_data.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow(['Movie', 'Rating', 'Slug'])\n",
    "    # Write data\n",
    "    writer.writerows(unique_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed847f",
   "metadata": {},
   "source": [
    "<h2>Extracting Reviews + Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into a pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"metacritic_data.csv\")\n",
    "\n",
    "titles_list = df['Movie'].tolist()\n",
    "slugs_list = df['Slug'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_reviews = []\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def scrape_critic_reviews(movie):\n",
    "    print(movie)\n",
    "    \n",
    "    url = f'https://www.metacritic.com/movie/{movie}/critic-reviews/'\n",
    "    \n",
    "    #Make a request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html_content = response.content\n",
    "\n",
    "    #Use regular expression to find occurrences of the target phrase and extract text in quotations\n",
    "    target_phrase = \"reviewPath:b,quote\"\n",
    "    pattern = re.compile(fr'{re.escape(target_phrase)}\\s*:\\s*[\"\\']([^\"\\']+?)[\"\\']')\n",
    "    matches = pattern.findall(html_content.decode('utf-8'))\n",
    "\n",
    "    #Concatenate the matches into a single string\n",
    "    concatenated_quotes = ' '.join(matches)\n",
    "\n",
    "    critic_reviews.append(concatenated_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in slugs_list:\n",
    "    scrape_critic_reviews(movie)\n",
    "    \n",
    "df['Critic Reviews'] = critic_reviews\n",
    "df.to_csv(\"movies_and_critic_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a00c74",
   "metadata": {},
   "source": [
    "<h1>LetterBoxd Scrapers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ad595",
   "metadata": {},
   "source": [
    "<h2>Extracting Movie Information + Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "letterboxd_data = []\n",
    "\n",
    "def extract_letterboxd_movie_info(movie_slug):\n",
    "    print(movie_slug)\n",
    "    \n",
    "    # Construct  URL\n",
    "    url = f'https://letterboxd.com/film/{movie_slug}/'\n",
    "\n",
    "    # Make request to URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract rating\n",
    "        rating_match = re.search(r'\"ratingValue\":\\s*(\\d+\\.\\d+)', str(soup))\n",
    "        rating = float(rating_match.group(1)) if rating_match else None\n",
    "\n",
    "        # Extract genres\n",
    "        genres_match = re.search(r'\"genre\":\\s*\\[([^\\]]*)\\]', str(soup))\n",
    "        genres = [genre.strip('\"\\', ') for genre in genres_match.group(1).split(',')] if genres_match else None\n",
    "\n",
    "        # Extract start date\n",
    "        start_date_match = re.search(r'\"startDate\":\\s*\"(\\d{4})\"', str(soup))\n",
    "        start_date = start_date_match.group(1) if start_date_match else None\n",
    "\n",
    "        # Extract director\n",
    "        director_match = re.search(r'\"director\":\\s*\\[{\"@type\":\"Person\",\"name\":\"([^\"]+)\"', str(soup))\n",
    "        director = director_match.group(1) if director_match else None\n",
    "\n",
    "        # Extract first two actors\n",
    "        cast_list = soup.find('div', class_='cast-list text-sluglist')\n",
    "        actors_match = re.findall(r'href=\"/actor/([^/]+)/\"', str(cast_list))\n",
    "        actors = [actor.replace('-', ' ').title() for actor in actors_match[:2]]\n",
    "\n",
    "        # Extract studios\n",
    "        studios_section = soup.find('h3', string='Studios')\n",
    "        if studios_section:\n",
    "            # Find all studio links within the section\n",
    "            studio_links = studios_section.find_next('div', class_='text-sluglist')\n",
    "            if studio_links:\n",
    "                studio_links = studio_links.find_all('a', class_='text-slug')\n",
    "                # Extract the text from each link\n",
    "                studios = [link.get_text(strip=True) for link in studio_links]\n",
    "            else:\n",
    "                studios = None\n",
    "        else:\n",
    "            studios = None\n",
    "\n",
    "        # Append movie information to global list\n",
    "        letterboxd_data.append([rating, genres, start_date, director, actors, studios])\n",
    "    else:\n",
    "        letterboxd_data.append([None, None, None, None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in slugs_list:\n",
    "    extract_letterboxd_movie_info(movie)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ed57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new pandas dataframe for Letterboxd data\n",
    "\n",
    "# Specify column names\n",
    "column_names = ['lb_rating', 'genre', 'year', 'director', 'actors', 'production_companies']\n",
    "\n",
    "# Create a DataFrame from the list of lists\n",
    "df2 = pd.DataFrame(letterboxd_data, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cbe462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "combined_df = pd.concat([df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dc843",
   "metadata": {},
   "source": [
    "<h2>Extracting Reviews + Data Cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccef1e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slugs_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Add the concatenated reviews to the global list\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mall\u001b[39m\u001b[38;5;241m.\u001b[39mappend(movie_reviews_concatenated)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m slug \u001b[38;5;129;01min\u001b[39;00m \u001b[43mslugs_list\u001b[49m:\n\u001b[1;32m     38\u001b[0m     scrape_reviews_for_movie(slug, num_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slugs_list' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Global list to store the extracted review text\n",
    "all = []\n",
    "\n",
    "def scrape_reviews_for_movie(slug, num_pages=3):\n",
    "    print(slug)\n",
    "    base_url = 'https://letterboxd.com/film'\n",
    "    movie_url = f'{base_url}/{slug}/reviews/by/activity'\n",
    "\n",
    "    # List to store reviews for the current movie\n",
    "    movie_reviews = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_url = f\"{movie_url}/page/{page}/\"\n",
    "        response = requests.get(page_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            reviews = soup.find_all('li', class_='film-detail')\n",
    "\n",
    "            for review in reviews:\n",
    "                body_text = review.find('div', class_='body-text')\n",
    "                if body_text:\n",
    "                    review_text = body_text.find('p')\n",
    "                    if review_text:\n",
    "                        review_text = review_text.get_text(strip=True)\n",
    "                        movie_reviews.append(review_text)\n",
    "\n",
    "    # Concatenate reviews for the current movie into a single string\n",
    "    movie_reviews_concatenated = '\\n'.join(movie_reviews)\n",
    "\n",
    "    # Add the concatenated reviews to the global list\n",
    "    all.append(movie_reviews_concatenated)\n",
    "\n",
    "for slug in slugs_list:\n",
    "    scrape_reviews_for_movie(slug, num_pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc07e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe\n",
    "combined_df['lb_reviews'] = all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
